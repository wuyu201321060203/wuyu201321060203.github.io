---
layout: post
title:  Raft一致性算法分析与总结
category: articles
tags: Paxos distributed-system consensus-algorithm
image:
    feature: head9.jpg
author:
    name:   WuYu
    avatar: bio-photo-alt.jpg
comments: true
share: true
---

####Raft简介

Raft是一个用于日志复制，同步的一致性算法。它提供了和Paxos一样的功能和性能，但是它的算法结构与Paxos不同。这使得Raft相比Paxos更好理解，并且更容易构建实际的系统。为了强调可理解性，Raft将一致性算法分解为几个关键流程（模块），例如选主，安全性，日志复制，通过将分布式一致性这个复杂的问题转化为一系列的小问题进而各个击破的方式来解决问题。同时它通过实施一个更强的一致性来减少一些不必要的状态，进一步降低了复杂性。Raft还包括了一个新机制，允许线上进行动态的集群扩容，利用有交集的大多数机制来保证安全性。

####一些背景知识

1. 一致性算法简单回顾
一致性算法允许一个集群像一个整体一样工作，即使其中一些机器出现故障也不会影响正常服务。正因为如此，一致性算法在构建可信赖的大型分布式系统中都扮演着重要的角色。Paxos算法在过去10年中统治着这一领域：绝大多数实现都是基于Paxos，同时在教学领域讲解一致性问题时Paxos也经常拿来作为范例。但是不幸的是，尽管有很多工作都在试图降低它的复杂性，但是Paxos仍然难以理解。并且，Paxos自身算法的结构不能直接用于实际系统中，想要使用必须要进行大幅度改变，这些都导致无论在工业界还是教育界，Paxos都让人很DT。

时势造英雄，在这样的背景下Raft应运而生。Raft算法使用了一些特别的技巧使得它易于理解，包括算法分解（Raft主要分为选主，日志复制和安全三个大模块），同时在不影响功能的情况下，减少复制状态机的状态，降低复杂性。Raft算法或多或少的和已经存在的一些一致性算法有着相似之处，但是也具有如下特征：
- 强leader语义：相比其他一致性算法，Raft使用增强形式的leader语义。举个例子，日志只能由leader复制给其它节点。这简化了日志复制需要的管理工作，使得Raft易于理解。
- leader的选择：Raft使用随机计时器来选择leader，它的实现只是在心跳机制(任何一致性算法中都必须实现)上多做了一点“文章”，不会增加延迟和复杂性。
- 关系改变：Raft使用了一个新机制joint consensus允许集群动态在线扩容，保障Raft的可持续服务能力。

Raft算法已经被证明是安全正确的，同时也有实验支撑Raft的效率不比其他一致性算法差。最关键是Raft算法要易于理解，在实际系统应用中易于实现的特点使得它成为了解决分布式系统一致性问题上新的“宠儿”。

2. 复制状态机
一致性算法是从复制状态机中提出的。简单地讲，复制状态机就是通过彼此之间的通信来将一个集群从一个一致性状态转向下一个一致性状态，它要能容忍很多错误情形。经典如GFS，HDFS都是使用了单独的复制状态机负责选主，存储一些能够在leader crash情况下进行恢复的配置信息等，比如Chubby和ZooKeeper。

复制状态机的典型实现是基于复制日志，如下图所示：

每个server上存储了一个包含一系列指令的日志，这些指令，状态机要按序执行。每一个日志在相同的位置存放相同的指令（可以理解成一个log包含一堆entry，这些entry组成一个数组，每个entry是一个command，数组相同偏移处的command相同），所以每一个状态机都执行了相同序列的指令。一致性算法就是基于这样一个简单的前提：集群中所有机器当前处于一个一致性状态，如果它们从该状态出发执行相同序列的指令走状态机的话，那么它们的下一个状态一定一致。但由于分布式系统中存在三态（成功，失败，无响应），如何来确保每台机器上的日志一致就非常复杂，也是一致性算法需要解决的问题。通常来讲，一致性算法需要具有以下特征：
- 安全性：在非拜占庭故障下，包括网络分区，延迟，丢包，乱序等等情况下都保证正确。
- 绝对可用：只要集群中大多数机器正常，集群就能错误容忍，进行正常服务。
- 不依赖时序保证一致性：由于使用逻辑时钟，所以物理时钟错误或者极端的消息延迟都不影响可用性。
-  通常情况下，一个指令可以在一轮RPC周期内由大多数节点完成，宕机或者运行速度慢的少数派不影响系统整体性能。

####Raft算法

前面对Raft算法进行了简要的介绍，这里开始对它进行深入分析。Raft实现一致性的机制是这样的：首先选择一个leader全权负责管理日志复制，leader从客户端接收log entries，将它们复制给集群中的其它机器，然后负责告诉其它机器什么时候将日志应用于它们的状态机。举个例子，leader可以在无需询问其它server的情况下决定把新entries放在哪个位置，数据永远是从leader流向其它机器。一个leader可以fail或者与其他机器失去连接，这种情形下会有新的leader被选举出来。

通过leader机制，Raft将一致性难题分解为三个相对独立的子问题：

(1) Leader选举：当前leader跪了的情况下，新leader被选举出来。
(2) 日志复制：leader必须能够从客户端接收log entries，然后将它们复制给其他机器，强制它们与自己一致。
(3) 安全性：如果任何节点将偏移x的log entry应用于自己的状态机，那么其他节点改变状态机时使用的偏移x的指令必须与之相同。

1. Raft基本知识
一个Raft集群包含单数个server，5个是一个典型配置，允许该系统最多容忍两个机器fail。在任何时刻，每个server有三种状态：leader，follower，candidate。正常运行时，只有一个leader，其余全是follower。follower是被动的：它们不主动提出请求，只是响应leader和candidate的请求。leader负责处理所有客户端请求（如果客户端先连接某个follower，该follower要负责把它重定向到leader）。第三种状态candidate用于选主。下图展示了这些状态以及它们之间的转化：

Raft将时间分解成任意长度的terms，如下图所示：

terms有连续单调递增的编号，每个term开始于选举，这一阶段每个candidate都试图成为leader。如果一个candidate选举成功，它就在该term剩余周期内履行leader职责。在某种情形下，可能出现选票分散，没有选出leader的情况，这时新的term立即开始。Raft确保在任何term都只可能存在一个leader。term在Raft用作逻辑时钟，有了term servers可以判断一些过时的信息：比如过时的leader。每台server都存储当前term号，它随时间单调递增。term号可以在任何server通信时改变：如果某台server的当前term号小于其它servers，那么这台server必须更新它的term号，与它人保持一致；如果一个candidate或者leader发现自己的term过期，它就必须要“放下身段”变成follower；如果某台server收到一个过时的请求（拥有过时的term号），它会拒绝该请求。Raft servers使用RPC交互，基本的一致性算法只需要两种RPC。RequestVote RPCs由candidate在选举阶段发起。AppendEntries RPCs在leader复制数据时发起，leader在和其他人做心跳时也用该RPC。servers发起一个RPC，如果在一定时间内没得到响应，需要重试。另外，发起RPC是并行的。

2. leader选举
Raft使用心跳机制来触发选举。当server启动时，初始状态都是follower。每一个server都有一个定时器，超时时间为election timeout，如果某server没有超时的情况下收到来自leader或者candidate的任何RPC，定时器重启，如果超时，它就开始一次选举。leader给其他人发RPC要么复制日志，要么就是用来告诉followers老子是leader，你们不用选举的心跳（告诉followers对状态机应用日志的消息夹杂在心跳中）。如果某个candidate获得了大多数人的选票，它就赢得了选举成为新leader。每个server在某个term周期内只能给最多一个人投票，按照先来先给的原则。新leader要给其他人发送心跳，阻止新选举。

在等待选票过程中，一个candidate，假设为A，可能收到它人的声称自己是leader的AppendEntries RPC，如果那家伙的term号大于等于A的，那么A承认他是leader，自己重新变成follower。如果那家伙比自己小，那么A拒绝该RPC，继续保持candidate状态。

还有第三种可能性就是candidate既没选举成功也没选举失败：如果多个followers同时成为candidate去拉选票，导致选票分散，任何candidate都没拿到大多数选票，这种情况下Raft使用超时机制来解决。Raft给每一个server都分配一个随机长度的election timeout（一般是150——300ms），所以同时出现多个candidate的可能性不大，即使机缘巧合同时出现了多个candidate导致选票分散，那么它们就等待自己的election timeout超时，重新开始一次新选举（再一再二不能再三再四，不可能每次都同时出现多个candidate），实验也证明这个机制在选举过程中收敛速度很快。

3. 日志复制

一旦选举出了一个leader，它就开始负责服务客户端的请求。每个客户端的请求都包含一个要被复制状态机执行的指令。leader首先要把这个指令追加到log中形成一个新的entry，然后通过AppendEntries RPCs并行的把该entry发给其他servers，其他server如果发现没问题，复制成功后会给leader一个表示成功的ACK，leader收到大多数成功后应用该日志，返回客户端执行结果。如果followers crash或者丢包，leader会在一定时间内重试AppendEntries RPC。Logs按照下图组织：

每个log entry都存储着一条用于状态机的指令，同时保存从leader收到该entry时的term号。该term号可以用来判断一些log之间的不一致状态。每一个entry还有一个index指明自己在log中的位置。

leader需要决定什么时候将日志应用给状态机是安全的，被应用的entry叫committed。Raft保证committed entries持久化，并且最终被其他状态机应用。一个log entry一旦复制给了大多数节点就成为committed。同时要注意一种情况，如果当前待提交entry之前有未提交的entry，即使是以前过时的leader创建的，只要满足已存储在大多数节点上就一次性按顺序都提交。leader要追踪最新的committed的index，并在每次AppendEntries RPCs（包括心跳）都要捎带，以使其他server知道一个log entry是已提交的，从而在它们本地的状态机上也应用。