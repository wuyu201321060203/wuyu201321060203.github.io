---
layout: post
title:  透过PacificA看分布式系统强一致性
category: articles
tags: strong-consistency distributed-system PacificA

image:
     feature: head19.jpg

author:
     name:   WuYu
     avatar: bio-photo-alt.jpg

comments: true
share: true
---

####PacificA简介

PacificA是微软为大规模分布式存储系统开发的一个通用复制框架，也可称之为一个原型系统。该框架简单，实用，提供强一致性，并且可以适配不同的复制策略。它对于我们理解分布式系统的强一致性，容错，仲裁有很好的指导意义。

####为什么开发PacificA

随着信息量的急剧增长，大规模的分布式存储系统变得越来越重要。为了提高性价比，这些系统往往采用廉价的商用机器或硬件，失效出错成为了分布式存储系统的常态，如何容错是这类系统实现可用性和可靠性的关键。众所周知并且已然被证明是正确的的复制协议“大有人在”，但是理论距离现实还是有一定差距的，理论可以不管不顾消息在网络中传递的次数，系统性能，吞吐量，系统设计的难易问题等等，但是真实的系统设计必须以整体的性能为根本依据，因此如何权衡理论和现实系统，设计一个可用，正确，高效的复制协议变得尤为重要。这个难题同样摆在了微软的面前，相比于单纯为了需求而设计一个具体的，特定的复制协议算法，微软高明地选择设计一个简单，实用，具有普适适用性的复制框架，这样不同的策略都可借助于该框架实现，并且有助于不同的策略优劣的比较。

####PacificA的设计特点

- 复制配置管理和数据复制分离，Paxos负责管理配置，主副本策略负责数据复制
- 将错误检测和配置更新容在数据复制的交互里面，去中心化，降低瓶颈
- 设计一个通用和抽象的框架模型容易验证正确性，实现不同的策略实例

这里再次验证了那个大白话的软件设计准则——实现依赖抽象，谁抽象的合理，优秀谁家的系统设计的就会更好。

####PacificA核心机制

#####前提与假设

1. 节点可以失效，对消息延迟的上限不做假设，即异步系统。
2. 消息可以丢失，乱序，但不能被篡改，即不存在拜占庭问题。
3. 网络分区可以发生，系统时钟可以不同步，但是漂移是有限度的。

#####名词解释

每份数据可以存储在多个节点上，针对同一份数据，这些节点组成一个“复制组”。复制组中得每个节点叫副本，其中一个副本称为primary，其他叫做secondaries，数据的复制遵循主副本策略。一个复制组是如何组成的，即谁是主，谁是secondaries这种事情叫复制组的配置。复制组配置是可以改变的，会有版本号去记录追踪变化。

#####主副本复制策略

所有的外部请求都走primary副本，如果是更新操作，需要primary带动其他secondaries。通过单点读写更容易控制副本一致性问题，当然你如果说效率，那这个可以通过数据分片，异构存储来解决，不在本文的讨论范围。什么是强一致性？通俗但我觉得不失准确地理解是对外部而言好像不存在副本一样，即对外部系统而言副本存在是透明的，这要求我们必须做到两点，（1）所有副本的更新顺序保持一致。（2）不能读到过期数据。想实现强一致性，需要primary副本控制和维护更新的顺序并且指导其他副本按顺序完成该操作。PacificA的每个副本需要维护一个被称为"prepared list"的请求列表和一个叫做"committed point"提交点的东西。这个prepared list是按照请求序列号排序的一个存储外部请求的列表，它必须是连续的（向列表中插入序列号为sn的请求必须在sn-1请求插入之后），请求序列号当然是我们primary副本决定的了。prepared list从开始一直到committed point为止的这么个列表前缀被称为"committed list"。primary副本的节点状态（应用看到的状态）可以抽象的定义为从初始状态开始按，照序列号递增的顺序应用committed list中存储的更新请求的结果。我们用$committed_p$和$prepared_q$分别代表p节点的committed list和prepared list。查询操作非常简单，primary根据自己的当前状态直接回答即可。对于更新操作，primary首先分配序列号，然后将该更新操作连同配置版本，序列号以prepare消息的形式一起发给所有secondaries。当某个副本收到该prepare消息，它就将该请求按照序列号顺序插入prepared list中，此时认为该请求已经在该副本中准备好了，然后该副本回复给primary一个ACK。当primary收到所有secondaries的ACK后这个消息就可以认为是commited（已提交）的了，primary向前移动committed point到适当位置，即从这开始向前的请求都是commited的。primary然后就可以回复客户端更新成功完成。对每一个prepare消息，primary稍后会向secondaries发送一个commit通知，告诉它们自己的committed point位置，secondaries收到通知后根据指示移动committed point到相同的位置。

因为primary只有在所有secondaries将请求添加进prepared list之后才可以通过采用移动committed point的方式将该请求插入committed list中，因此primary的committed list肯定是任何一个副本的前缀。同时，因为一个secondary只有在primary将一个请求添加进committed list后才会把同样地请求添加进committed list中，因此一个secondary上的committed list肯定是primary上committed list的前缀，这就存在一个复制协议必须要遵守的不变式，此不变式称为Commit Invariant。

**Commit Invariant:** 让p和q分别代表当前复制组配置中的primary和任何一个其他副本，存在$committed_q\subseteq$ $committed_p\subseteq$  $prepared_q$

这个基本的复制协议只在复制组配置不变的情况下才有效。

#####配置管理

在PacificA中有一个全局的配置管理器（以下简称为manager）负责管理所有复制组的配置。当一个节点怀疑其他副本有问题的时候可以向manager提出重新配置的请求，这时如果manager接受此请求，则会从当前配置中移除有问题的副本。当然，一个节点也可以提出为当前复制组添加副本的请求。每次请求重新配置的时候都需要附带当前配置版本号，只有这个版本号和manager记录的一致才会被执行，如果请求成功，这个新配置会被赋予新的版本号。这个很好理解就是为了防止配置过时还来更新的情况，只有即时的配置才会被更新。

在发生网络分区的情况下，有可能发生重新配置请求产生冲突的情况：primary要求移除某些secondaries，而某些secondaries试图取代primary，这时manager采取的策略很简单——先到先得。任何满足以下被称为Primary Invariant的错误检测协议都可以被用来实现移除副本的功能。

**Primary Invariant:** 在任何时刻，一个节点p只有在manager认为它是primary时才能认为自己是primary。说白了就是谁是primary这事得manager说的算，这保证一个复制组任何时刻都至多有一个primary。

#####租约机制与错误检测

即便只有manager全权管理复制组配置，Primary Invariant有时还是不能满足。因为每个节点对配置的认识并不是始终同步的，即存在“双活问题”，老的primary和新的primary同时行使权力。如果存在这种现象，强一致性是没办法保证的。这里采用一个被称为租约机制的东西来解决问题。primary副本需要定期联系其复制组的其他副本维护租约，如果从上次收到租约回复起超过*lease period*时间段内没有收到某一个secondary的租约回复，这个primary就不能认为自己是primary了，它必须停止执行一切更新和查询服务，这时primary会向manager请求移除它认为出问题的副本。

一个secondary收到当前配置中它认可的primary的租约请求就会立马回复，如果从上次收到primary的租约请求开始*grace period*时间段内再没有收到同样的请求，那么这个secondary就可以认为这个primary跪了，并且请求manager干掉它，自己取而代之。假设没有时钟漂移，只要grace period大于等于lease period，那么租约机制可以保证primary副本要比任何一个secondary先感知到租约失效。而同时，任何一个secondary只有在它判定租约失效时才会争取去当新的primary，因此不可能再同时存在两个primary，Primary Invariant被hold住了。

这里有个有趣的问题就是像GFS，Bigtable那些也有租约机制，但是是一个具有中心化的实体比如master节点来触发和维护的，但这里没有那么做，而是选择在本来就有信息交互的主副本间进行的，这样可以用主副本本来就进行的普通数据交互信息充当租约信息，最小化了错误检测的代价，降低系统瓶颈。

#####重新配置，仲裁与故障恢复

复制协议一个比较复杂的地方就在于如何处理配置发生变化的情况。PacificA将配置变化分为三个情况：

**Removal of Secondaries.** 当primary认为一个或多个secondary跪了的时候就可以触发移除副本操作。primary向manager提出一个更新配置请求，该配置不再含有被认为有问题的副本，如果manager同意了，那么以后primary就可以把那些被认为有问题的副本当做空气来执行操作流程了。

**Change of Primary.** 当一个secondary采用租约机制后认为当前primary挂了的时候就可以触发移除primary，它同样会向manager请求更新配置，该配置中把老的primary剔除了，自己要求成为新的primary。如果manager同意，那么该secondary就可以认为自己是primary，翻身做主了。但要注意，它需要在执行一个叫做”仲裁“的过程后才能行使primary处理请求的责任，这个仲裁过程是这样的：假设副本p认为自己是新primary了，它需要向所有其他secondaries发送prepare消息，期望让自己身上还没有committed的请求能够committed掉。假设*sn*是仲裁后p的committed point，然后p指示其他副本截断prepared lists，最多只允许保留到序列号sn。

通过前面我们知道一个副本的prepared list肯定覆盖所有已经committed的请求，仲裁过程保证了被称为*Reconfiguration Invariant*的不变式。

**Reconfiguration Invariant:** 如果一个新副本p在时刻t完成仲裁，那么任何其他副本在早于t的任何时刻时维护的committed list肯定是时刻t时p的committed list的一个前缀，这样在新配置中Commit Invariant依然成立。说起来挺绕，有图有真相，如图Figure 1所示：

Figure 1展示了当primary发生改变时prepared list和committed points是如何改变的。开始A是primary，B，C，D是secondary。注意$committed_B$是$committed_A$的前缀，$committed_A$也是任何其他副本prepared list的前缀。假设如果发生了一次配置更新，B取代A成为了新primary，这时B需要完成重新仲裁的过程，新的$committed_B$会和老的$prepared_B$一样，因此也会覆盖$committed_A$。$prepared_C$会被更新而有多余东西的$prepared_D$会被截断来维护Commit Invariant。

**Addition of New Secondaries.** 有些副本出问题的时候，为了保持副本数量会触发添加副本的操作。要注意添加新副本的时候也要维护Commit Invariant，这要求新副本在加入某复制组前需要“合适的”prepared lists，这个新副本获取正确状态的过程被称为*recovery*。

一个简单地recovery方式就是primary啥也不干了，直到这个被添加的新副本从某个其他副本那拷贝完prepared list为止，这个肯定影响服务质量，因此被pass掉。取而代之的是新副本以一个*candidate* secondary的身份加入，primary可以继续处理更新，也会向candidate发送一份prepare消息，当然，如果这个新副本失联了，那么primary会立马结束它candidate的身份。

除了为primary发过来prepare消息记录更新请求外，一个candidate secondary *c*也要从某一个前辈副本那获取prepared list的正确部分。当*c*最终追上进度的时候，它可以请求primary把自己加入当前配置中，这时primary会联系manager把*c*加进来，*c*的身份也会随之变成普通secondary。

如果是添加一个从来没有在这个复制组出现过的副本，完整地状态迁移，进度追赶是不可避免的。但是有时候也可能是由于网络分区，暂时性错误引发的“不肖子孙，迷途知返”，这时候只需要补上丢失的更新就可以了。一个副本在离开当前复制组一段时间后重新回来，根据Reconfiguration Invariant，它离开前的committed list肯定是回来后的committed list和prepared list的前缀，但是没法保证它离开前的prepared list是回来后prepared list的前缀，因此这个副本回来后需要舍弃committed point之后的东西，去其他前辈副本那补回来。

#####正确性论述

前面讲解的复制协议在一个复制组中只要至少有一个正常工作的副本的情况下便可以实现强一致性，这里从线性化，持久性和保活性三个方面对其进行正确性论述。首先解释线性化，持久性和保活性三个名词：

**Linearizability:** 系统对所有请求的执行等同于一个线性化执行的结果。
**Durability:** 如果一个客户端收到系统对于某个更新的回复，那么该更新一定包含在了这个等同的线性化执行结果里。
**Progress:** 我把它称为保活性，假设正常工作的节点间通信链路可依赖，manager会回复每一个更新配置请求，那么系统最终会进入一个正常，稳定的工作状态，它会回复每一个客户端的请求。

对于线性化，我们知道primary维护的committed list唯一地决定了更新的执行顺序，而Primary Invariant决定了任何时刻不会同时有两个primary提供服务，而即使在primary发生改变的时候，根据Reconfiguration Invariant，老primary的committed list也是新primary的committed list的前缀，因此所有副本的更新顺序保持了一致。查询服务是在primary单点上进行的，primary的状态由其committed list决定，显而易见数据是实时的，因此线性化被保证。

对于持久性，如果一个客户端收到了更新回复，代表这个primary肯定已经将该请求插入到了committed list中了，结果肯定是持久有效的。

对于保活性，假设manager最终会回复每一个更新配置的请求，系统最终会进入一个正常稳定的工作状态，那么所有的请求肯定都会被系统执行和回复。

####总结

PacaficA向我们展示了一个实现强一致性的标准复制协议具有的特征，对于我们理解分布式系统的强一致性有很好的指导意义。曾经我天真地认为GFS是一个强一致性的系统，通过PacaficA我才知道它并非如此:-）
